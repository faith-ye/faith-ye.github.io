<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自注意力模型</title>
      <link href="/2022/04/02/zi-zhu-yi-li-mo-xing/"/>
      <url>/2022/04/02/zi-zhu-yi-li-mo-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>前面我们学习过LSTM、GRU，它们都可以挖掘序列之间的某种联系。举个简单的例子——I saw a saw（我看见了一把锯子），句中两个saw无论在词义还是词性中都有所不同。如果将这句话简单做词向量处理，然后丢进一个全连接模型的话，那么两个saw输出的结果是一样的。因为对于这种模型而言的话，它是挖掘不出词与词之间的关系。而对于LSTM，GRU来说，它通过一定的机制可以学习到句子和句子之间的联系。</p><p>那么注意力机制是怎么学习这种联系的呢？这还得从我们人类的视觉说起。当我们在看到图片或风景的时候，我们会将注意力集中到我们关注的那些事物上。比如你在绘画的过程中，你会持续地关注你构思到画板上的元素（比如蓝天，白云），而不会太多关注那些其他的元素，比如风，虫鸣，阳光等等。这种有意识的聚焦就被称为注意力机制。那么机器是怎么将这种机制应用到模型中呢？这也是这篇文章要学习的内容。</p><h2 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h2><p>仍然以I saw a saw为例，如下图所示，我们设置一个window，该window只考虑了周围三个输入，此时模型当前的输出就和周围三个输入有关。那么我们将window覆盖整个文本，是不是可以考虑整个句子中单词与单词之间的联系？</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402122749713-16488871089911.png" class="" title="image-20220402122749713"><p>自注意力机制借鉴了这个想法，用一个结构实现上述Window中的操作，从而可以考虑句中每个单词之间的联系，进而区别出这两个saw。</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402124424005-16488871089922.png" class="" title="image-20220402124424005"><p>Attention机制通常有Bahdanau Attention（右图）与Luong Attention（左图），两种注意力的理论相似，Luong Attention的使用范围更广泛，因此本文主要讲解Luong Attention。在讲解Luong Attention前，我们先来讲解三个概念——查询、键和值。</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402125443414-16488871089923.png" class="" title="image-20220402125443414"><h3 id="查询、键和值"><a href="#查询、键和值" class="headerlink" title="查询、键和值"></a>查询、键和值</h3><p>在注意力机制的背景下，我们将自主性提示称为查询（query）。给定任何查询，注意力机制通过注意力汇聚将选择引导至感官输入。在注意力机制中，这些感官输入被称为值（value）。对于每个值都有一个键（key）与之配对，这可以想象成感官输入的非自主提示。通过注意力汇聚，每个查询（自主性提示）都可以与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。下面我们来看看查询、键和值是怎样在self-attention中运作的。</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402132249578-16488871089924.png" class="" title="image-20220402132249578"><p>  假设输入是：${a^{1},a^{2},a^{3},a^{4}}$。每个输入都对应查询、键和值。查询$q^{i}=W^{q}a^{i}$，键$k^{i}=W^{k}a^{i}$以及值$v^{i}=W^{v}a^{i}$。对于查询query，我们需要找所对应的键与之进行匹配，这样就可以得出那些信息比较重要。对应计算$a_{1,i}=q^{1}k^{i}$，再经过softmax层就可以算出每个信息对应的比重$a^{‘}<em>{1,i}=exp(a</em>{1,i}/\sum_{j}(a_{1,j}))$，进而可以求出$a^{1}$对应的输出$b^{1}=\sum_{i}a^{‘}_{1,i}v^{i}$，同理我们可以计算出$b^{2},b^{3},b^{4}$。如果计算机也这样一个接一个计算，那计算效率太低。其实我们可以通过矩阵运算实现平行运算，具体操作如下：</p><p>查询：$$q^{i}=W^{q}a^{i}\Rightarrow (q^{1},q^{2},q^{3},q^{4})=W^{q}(a^{1},a^{2},a^{3},a^{4})$$</p><p>键：$$k^{i}=W^{k}a^{i}\Rightarrow (k^{1},k^{2},k^{3},k^{4})=W^{k}(a^{1},a^{2},a^{3},a^{4})$$</p><p>值：$$v^{i}=W^{v}a^{i}\Rightarrow (v^{1},v^{2},v^{3},v^{4})=W^{v}(a^{1},a^{2},a^{3},a^{4})$$</p><p>对于中间部分的计算可以用下图表示：</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402144505171-16488871089925.png" class="" title="image-20220402144505171"><p>从而可以得到输出：</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402144618597-16488871089926.png" class="" title="image-20220402144618597"><p>整理一下可得：</p><p>查询、键和值：$$Q,K,v=W^{q}I,W^{k}I,W^{v}I$$</p><p>注意力矩阵：$$A^{‘}\Leftarrow A=K^{T}Q$$</p><p>输出：$$O=VA^{‘}$$</p><p>由此可以看出 self-attention 需要率定的参数有$W^{q},W^{k},W^{v}$</p><h3 id="多头自注意力模型"><a href="#多头自注意力模型" class="headerlink" title="多头自注意力模型"></a>多头自注意力模型</h3><p>多头注意力（Multi-head Self-attention）模型是建立在自注意力模型的基础上。它模拟的是序列中存在不止一种的联系，这时单靠一个head是无法捕捉序列中的完整信息。以2 head为例，利用两组$W^{q},W^{k},W^{v}$对应输入$a^{i}$分别单独计算出两个输出$b^{i,1},b^{i,2}$，然后通过一个输出矩阵可以得出$b^{i}$：</p><p>$$b^{i}=W^{o}(b^{i,1},b^{i,2})^{T}$$</p><p>由此可以看出2 head带来了两倍以上的参数，虽然模型的准确度得到了提升，但是以损失计算能力为代价。</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>self-attention虽然考虑了输入序列中每个成分之间的联系，但并没考虑输入序列的先后顺序。这是因为self-attention中的计算是平行计算，无论序列中两个成分相隔多远，对self-attention的整个计算没有什么影响。而对于某些实际应用，序列的顺序对模型影响很大或者可以一定程度上提升模型性能。举个例子，在词性标注中，我们知道动词是很少出现在一个句子的开头。所以当一个单词出现在句子的开头时，我们有很大的把握判断这个单词不是动词。</p><p>为了改进self-attention这个弱点，我们可以对输入进行一定的操作——位置编码，从而使得self-attention考虑到序列的顺序。这个操作其实很简单，我们只需在每个输入对应的位置加一个独一无二的位置向量即可实现：</p><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402153801462-16488871089927.png" class="" title="image-20220402153801462"><p>这时你就会有一个疑问，位置编码是怎么确定的？具体可以看这篇论文：<a href="https://arxiv.org/abs/2003.09229">Learning to Encode Position for Transformer with Continuous Dynamical Model</a></p><h2 id="Self-attention-vs-RNN"><a href="#Self-attention-vs-RNN" class="headerlink" title="Self-attention vs RNN"></a>Self-attention vs RNN</h2><img src="/2022/04/02/zi-zhu-yi-li-mo-xing/image-20220402155319292-16488871089928.png" class="" title="image-20220402155319292"><p>上图展示的是循环神经网络和自注意力模型的简易结构，由此可以看出self-attention相对RNN的结构优点：</p><ul><li>self-attention是平行计算，单次迭代计算速度块</li><li>self-attention可以方便地考虑两个相隔较远的单词之间的联系，而RNN虽然也能考虑到，但RNN在传递的过程中，这种联系会消失。因此self-attention在处理序列中含有较大关联的模型中更有优势。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php">李宏毅 机器学习2021</a></p><p>[2] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2Seq</title>
      <link href="/2022/03/29/seq2seq/"/>
      <url>/2022/03/29/seq2seq/</url>
      
        <content type="html"><![CDATA[<p>**写在前面：**这个部分主要记录一些关于深度学习相关论文的阅读，由于个人还是刚接触深度学习不久，所以前面需要补充一些很早的论文以巩固自己知识的不足。今天记录最近学的一篇论文<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>。这篇论文研究的是机器翻译领域，作者使用的方法是利用一个多层的 LSTM 将输入文本编码成一个向量，这个向量可以视为整个输入句子的抽象表示。然后用另一个 LSTM 将前面编码的向量解码成目标句子。作者将其应用在 WMT’14 数据集上英语到法语的翻译任务，并在整个测试集上得到的 BLEU score 为34.8。下面我门结合<a href="https://d2l.ai/">Dive into Deep Learning</a>这本书的相关章节和这篇论文了解 seq2seq 的一般概念，并陈述构建模型时需要用的一些方法。</p><h2 id="seq2seq">seq2seq</h2><p>最常用的 seq2seq 模型其实就是encoder-decoder模型。encoder-decoder模型通常使用 RNN 将一段文本作为输入编码（encoder）成一个向量，这个向量可以视为整个输入句子的抽象表示。然后，该向量通过第二个 RNN 解码（decoder），该 RNN 通过一次生成一个单词来学习目标句子（也就是另一种语言的句子）。下图演示了将 seq2seq 模型用于英文到中文的翻译。</p><img src="/2022/03/29/seq2seq/seq2seq.png" class=""><h3 id="Encoder">Encoder</h3><p>图中展示了一个简单的翻译模型，模型中用&lt;bos&gt;和&lt;eos&gt;分别表示开始词元和结束词元。它的输入句子是”good morning“，它首先通过embedding层转换为对应的词向量，然后再进入编码器（encoder）。在每一个时间步，进入编码器RNN含有embedding$x_{t}$和上一时间步隐状态$h_{t-1}$，然后编码器RNN产生新的隐状态$h_{t}$。我们可以抽象地把这个隐状态代表为前面的词元。该时间步的计算可以用以下公式表达：<br>$$h_{t}=EncoderRNN(e(x_{t},h_{t-1}))$$</p><p>在这里，我们通常使用LSTM或GRU这样的term RNN。假设，输入$X={x_{1},x_{2},…,x_{T}}$，式中 $x_{1}$=&lt;bos&gt;，$x_{2}$=good，etc 。编码器初始的隐状态 $h_{0}$ 通常初始化为0或者已经学习好的参数。一旦最后的词元 $x_{T}$ 通过embedding层进入编码器RNN，我们利用最后的隐状态 $h_{T}$ 来作为文本向量，并把它赋值给z :$h_{T}=z$</p><h3 id="Decoder">Decoder</h3><p>现在我们拥有文本向量z，我们可以开始将其解码成目标句子——”早上好“。同样我们用<sos>和<eos>分别表示目标句子的开始词元和结束词元。在每一个时间步，解码器RNN的输入是当前词元$y_{t}$的embedding $d$和上一时间步的隐状态$s_{t-1}$。在解码器RNN，初始隐状态$s_{0}=z=h_{T}$，即解码器初始隐状态就是编码器的最终隐状态。我们同样用一个公式表示该时间步解码器的操作：<br>$$s_{t}=DecoderRNN(d(y_{t},s_{t-1}))$$<br>在解码器中，我们需要将隐状态转换为对应的单词，因此在每一个时间步，我们通过一个线性层通过$s_{t}$去预测下一个在文本出现的单词$\overset{-}{y_{t}}$<br>$$\overset{-}{y_{t}}=f(s_{t})$$<br>解码器中的单词是随着时间步一个接一个地生成。我们通常使用&lt;bos&gt;作为解码器的第一个输入$y_{1}$，但是对于接下来的输入$y_{t&gt;1}$，我们有时使用在目标句子中下一个单词，有时也会使用经解码器预测的下一个单词$\overset{-}{y_{t}}$，这在机器翻译中被叫做teacher forcing。使用teacher-forcing，在训练过程中，可以加快模型的训练，使得模型会有较好的效果。但是在测试的时候因为不能得到目标句子的支持，存在训练测试偏差，模型会变得脆弱。<br>训练模型时，我们通常知道目标句子有多少单词，一旦解码器输入目标单词，模型就会停止生成单词。但在测试模型时，解码器会不断生成单词，直到模型输出&lt;eos&gt;或生成一定数量的单词之后，模型就停止生成单词。</eos></sos></p><p>一旦模型得到了预测目标句子$\overset{-}{Y}={\overset{-}{y_{1}},\overset{-}{y_{2}},…,\overset{-}{y_{t}}}$，我们将其和目标句子$Y={y_{1},y_{2},…,y_{t}}$进行比较，计算出误差，利用该误差就可以更新模型的所有参数。</p><h2 id="技巧">技巧</h2><h3 id="数据预处理-2">数据预处理</h3><ul><li>将文本词元化，在机器翻译中我们更喜欢单词级词元化。对训练数据的文本序列进行词元，其中每个词元要么是一个词，要么是一个标点符号。</li><li>分别为源语言和目标语言构建两个字典——int2word 和 word2int （这两个字典将单词和整数一一对应）。同时为了减少数据噪声的影响，我们将出现次数少于2次的低频率词元视为未知 &lt;unk&gt; 词元。除此之外，我们还指定一些额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元 &lt;pad&gt; ，以及序列的开始词元 &lt;bos&gt; 和结束词元 &lt;eos&gt; 。</li><li>为了提高计算效率，我们可以通过截断和填充方式实现每个序列都具有相同的长度。当文本序列词元数目少于规定数据时，我们将继续在其末尾添加特定的 &lt;pad&gt; 词元。反之，我们将截断文本序列，只取前指定数目个词元，丢弃剩余词元。</li></ul><h3 id="搭建模型">搭建模型</h3><p>这一部分可以参考论文</p><img src="/2022/03/29/seq2seq/1.png" class=""><h2 id="参考资料-2">参考资料</h2><p>[1] <a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></p><p>[2] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>门控循环单元</title>
      <link href="/2022/03/25/men-kong-xun-huan-dan-yuan/"/>
      <url>/2022/03/25/men-kong-xun-huan-dan-yuan/</url>
      
        <content type="html"><![CDATA[<p>门控循环单元（gated recurrent units, GRU）于2014被Cho等人提出。GRU和LSTM一样有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态，但GRU没有单独的存储单元，即LSTM的记忆元。GRU是LSTM的一个变体，它结构更加简单，却能够提供和LSTM同等的效果，并且计算的速度明显更快。</p><p>现在我们从GRU的内部结构开始解读：</p><h2 id="重置门和更新门"><a href="#重置门和更新门" class="headerlink" title="重置门和更新门"></a>重置门和更新门</h2><p>我们首先介绍重置门（reset gate）和更新门（update gate）。重置门控制着我们还想记住过去状态的数量；更新门决定着新状态保存旧状态信息的程度。这两个门的输入和LSTM一样，是当前时间步的输入和上一时间步的隐状态，它们的输出是由使用sigmoid激活函数的两个全连接层给出。因此它们的数学表达为：</p><p>$$R_{t}=\sigma({X_{t}W_{xr}+H_{t-1}W_{hr}+b_{r}})$$</p><p>$$Z_{t}=\sigma({X_{t}W_{xz}+H_{t-1}W_{hz}+b_{z}})$$</p><p>式中：$X_{t}\in R^{nxd}$是输入（样本个数：n，维度：d）；$H_{t-1}\in R^{nxh}$是上一时间步的隐状态；$R_{t}\in R^{nxd}$和$Z_{t}\in R^{nxd}$分别是重置门和更新门；$W_{xr},W_{xz}\in R^{dxh}$和$W_{hr},W_{hz}\in R^{hxh}$是权重参数，$b_{r},b_{z}\in R^{1xh}$是偏置项。</p><h2 id="候选隐状态"><a href="#候选隐状态" class="headerlink" title="候选隐状态"></a>候选隐状态</h2><p>候选隐状态（candidate hidden state）$\overset{-}{H_{t}}\in R^{nxh}$的计算公式如下：</p><p>$$\overset{-}{H_{t}}=tanh(X_{t}W_{xh}+(R_{t}\bigodot H_{t-1})W_{hh}+b_{h})$$</p><p>式中$W_{xh}\in R^{dxh}$和$W_{hh}\in R^{hxh}$是权重参数，$b_{h}\in R^{1xh}$是偏置项，符号$\bigodot$是Hadamard积（按元素乘积）运算符。</p><p>重置门$R_{t}$可以控制以往状态的影响程度， 当$R_{t}$中所有项接近1时，保留前一隐状态所有影响。当$R_{t}$中所有项接近0时，候选隐状态是以$X_{t}$作为输入的多层感知机的结果。</p><p>以上计算流程可以用下图表示：</p><img src="/2022/03/25/men-kong-xun-huan-dan-yuan/1.png" class=""><h2 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h2><p>隐状态$H_{t}\in R^{nxh}$通过更新门$Z_{t}$来确定它多大程度上来自与旧的状态$H_{t-1}$和当前候选状态$\overset{-}{H_{t}}$，它的具体公式为：</p><p>$$H_{t}=Z_{t}\bigodot H_{t-1}+(1-Z_{t})\bigodot \overset{-}{H_{t}}$$</p><p>每当更新门$Z_{t}$中所有项接近1时，模型就倾向于只保存旧状态。此时，来自$X_{t}$的信息基本上被忽略。相反，当$Z_{t}$中所有项接近0时，新的隐状态$H_{t}$就会接近候选隐状态$\overset{-}{H_{t}}$这些设计可以帮助我们处理循环神经⽹络中的梯度消失问题，并更好地捕获时间步距离很⻓的序列的依赖关系。 </p><p>此时GRU一个神经元一个完整的内部结构就可以用下图表示：</p><img src="/2022/03/25/men-kong-xun-huan-dan-yuan/2.png" class=""><p>用LSTM，我们最后将最新的隐状态作为输入进入另一个网络，则可实现分类或回归等模型。</p><h2 id="GRU网络代码"><a href="#GRU网络代码" class="headerlink" title="GRU网络代码"></a>GRU网络代码</h2><p>同LSTM我们可以调用pytorch里面的API实现一个简单的GRU网络</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">GRU_Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> fix_embedding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        embedding: 词典        embedding_dim: 词向量的维度        hidden_dim: GRU神经元个数        num_layers: GRU的层数        output_dim: 隐藏层输出的维度(分类的数量)        """</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>GRU_Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 制作 embedding layer</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span>        <span class="token comment"># 如果 fix_embedding 为 False，在训练过程中，embedding 也会跟着被训练</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token keyword">if</span> fix_embedding <span class="token keyword">else</span> <span class="token boolean">True</span>        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span> bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 句子最后时刻的hidden state</span>        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用LSTM同样的例子，经过这个网络得出在验证集中准确率最高为0.9024，和LSTM模型的准确度差不多，但运行时间个人明显感觉短了很多。</p><h2 id="与LSTM间的异同"><a href="#与LSTM间的异同" class="headerlink" title="与LSTM间的异同"></a>与LSTM间的异同</h2><p>两者相似之处：引用了门结构，并在t-１时刻到ｔ时刻信息的传递引用了新的成分（候选隐状态，在LSTM中是记忆元），不再像传统RNN只利用了当前时刻的输入和上一时刻的隐状态。这个相同之处带来了两个好处：</p><p>①能够保存长期序列中的信息，且不会随时间而清除或因为与预测不相关而移除。</p><p>②有效创建了绕过多个时间步骤的快捷路径。这些捷径允许误差更容易反向传播，不至于像传统RNN那样迅速消散，从而解决了梯度消失的问题。</p><p>两者不同之处：</p><p>①对记忆内容传递程度的控制。LSTM用output gate控制传递程度，传递给下一个unit；而GRU是完全传递给下一个unit，不做任何控制。</p><p>②对候选内容的控制；LSTM计算候选记忆元不对上一信息做任何控制；而GRU计算候选隐状态时利用reset gate对上一时刻的信息进行控制。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://arxiv.org/abs/1412.3555">Chung J, Gulcehre C, Cho K H, et al. Empirical evaluation of gated recurrent neural networks on sequence modeling[J]. arXiv preprint arXiv:1412.3555, 2014.</a></p><p>[2] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>长短期记忆网络</title>
      <link href="/2022/03/22/chang-duan-qi-ji-yi-wang-luo/"/>
      <url>/2022/03/22/chang-duan-qi-ji-yi-wang-luo/</url>
      
        <content type="html"><![CDATA[<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><strong>长短期记忆神经网路</strong>（long short-term memory，LSTM）是一种RNN特殊类型，现在我们见到的RNN模型除了特别强调，一般都是LSTM。LSTM的设计灵感来源于计算机的逻辑门，它引入了记忆元（memory cell），记忆元的作用是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，我们将其称为输出门（output gate）。另一个门用来决定何时将数据读入单元，我们将其称为输入门（input gate）。我们还需要一个门来决定什么时候记忆或忽略隐状态中的输入，我们将其称为遗忘门（forget gate）。除此之外还有一个候选记忆元（candidate memory cell），它用来控制输入门的状态。现在让我们看看这在实践中是怎么运作的。<br>LSTM当前时间步的输⼊和前⼀个时间步的隐状态作为数据送⼊⻓短期记忆⽹络的⻔中，它们由三个具有sigmiod激活函数的全连接层处理，以计算输入门、输出门和遗忘门的值。候选记忆元的结构与前面三个门相似，只是使用tanh函数来作为激活函数。</p><img src="/2022/03/22/chang-duan-qi-ji-yi-wang-luo/1.png" class=""><p>现在我们来细化一下长短期记忆网络的数学表达。假设输入为$X_{t}\in R^{nxd}$，隐藏单元的个数是h，则前一时间步的隐状态为$H_{t-1}\in R^{nxh}$。相应地，输入门是$I_{t}\in R^{nxh}$，遗忘门是$F_{t}\in R^{nxh}$，输出们是$O_{t}\in R^{nxh}$，候选记忆元是$\overset{-}{C_{t}}\in R^{nxh}$。它们的计算方法如下：<br>$$I_{t}=\sigma(X_{t}W_{xi}+H_{t-1}W_{hi}+b_{i}),$$<br>$$F_{t}=\sigma(X_{t}W_{xf}+H_{t-1}W_{hf}+b_{f}),$$<br>$$O_{t}=\sigma(X_{t}W_{xo}+H_{t-1}W_{ho}+b_{o}),$$<br>$$\overset{-}{C_{t}}=tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c}),$$<br>式中$W_{xi},W_{xf},W_{xo},W_{xc}\in R^{dxh}$和$W_{hi},W_{hf},W_{ho},W_{hc}\in R^{hxh}$是权重参数，$b_{i},b_{f},b_{o},b_{c}\in R^{1xh}$是偏置参数。<br>在LSTM中，有两个来控制输入或遗忘：输入门$I_{t}$控制采用多少来自$\overset{-}{C_{t}}$的新数据，而遗忘门$F_{t}$控制保留多少过去的记忆元$C_{t-1}\in R^{nxh}$的内容。它们之间使用Hadamard积，有：<br>$$C_{t}=F_{t}\bigodot C_{t-1}+I_{t}\bigodot \overset{-}{C_{t}}$$<br>如果遗忘⻔始终为1且输⼊⻔始终为0，则过去的记忆元$C_{t}$将随时间被保存并传递到当前时间步。引⼊这种设计是为了缓解梯度消失问题，并更好地捕获序列中的⻓距离依赖关系。<br>现在，我们还需要定义如何计算隐状态$H_{t}\in R_{nxh}$，这就是输出门发挥作用的地方。在LSTM中，它将记忆元经过tanh激活函数，并于输出之间做Hadamard积：<br>$$H_{t}=O_{t}\bigodot tanh(C_{t})$$<br>这样我们就得到LSTM一个神经元完整的内部结构：</p><img src="/2022/03/22/chang-duan-qi-ji-yi-wang-luo/2.png" class=""><p>最后只需将最新的隐状态作为输入经过另一个网络，则可实现分类或回归等模型。</p><p>这只是LSTM单个神经元的内部结构，而事实上LSTM可以将多层网络结构堆叠在一起，每层网络结构含有多个神经元结构。通过对几个层进行组合，我们就可以产生一个灵活的网络结构。</p><h2 id="LSTM应用"><a href="#LSTM应用" class="headerlink" title="LSTM应用"></a>LSTM应用</h2><p>现在我们以一个新闻标题分类的例子来实现LSTM，本次实验应用的是清华NLP组提供的THUCNews文本分类数据集，它包含十个新闻主题，分别是财经、房产、股票、教育、科技、社会、时政、体育、游戏。训练集中总共有180000条数据，每个类别有200000条数据。验证集中总共有10000条数据，每个类别有1000条数据。</p><h3 id="分词及去除停用词"><a href="#分词及去除停用词" class="headerlink" title="分词及去除停用词"></a>分词及去除停用词</h3><p>由于训练数据是中文，因此需要对文本进行分词处理，分词采用的是python中的jieba工具。同时为了减少停用词对文本有效信息造成噪音干扰，减少模型复杂程度，我们对文本进行去除停用词处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> jieba<span class="token keyword">import</span> os<span class="token keyword">import</span> re<span class="token comment"># 数据所在路径</span>data_dir <span class="token operator">=</span> <span class="token string">'E:\软件包\Chrome\机器学习数据\THUCNews'</span><span class="token comment"># 停用词表对应路径</span>stopwords_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> <span class="token string">'中文停用词库.txt'</span><span class="token punctuation">)</span><span class="token comment"># 将停用词转换为python列表</span>stopwords_list <span class="token operator">=</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> <span class="token builtin">open</span><span class="token punctuation">(</span>stopwords_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'gbk'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment"># 定义一个函数，用于对文本分词和去除停用词处理</span><span class="token keyword">def</span> <span class="token function">text_preprocessing</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 去除文本额外信息</span>    text <span class="token operator">=</span> text<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'(图)'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>    <span class="token comment"># 只保留数字、字母、中文</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r'[^a-zA-Z0-9\u3002\uff1b\uff0c\uff1a\u201c\u201d\uff08\uff09\u3001\uff1f\u300a\u300b\u4e00-\u9fa5]'</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    <span class="token comment"># 分词</span>    words <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    text_list <span class="token operator">=</span> <span class="token punctuation">[</span>word<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> words <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> stopwords_list <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>word<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> text_list<span class="token keyword">def</span> <span class="token function">load_data</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 把 training 时需要的资料读进来</span>    f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>    lines <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>    lines <span class="token operator">=</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    x <span class="token operator">=</span> <span class="token punctuation">[</span>text_preprocessing<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h3><p>这里采用的时gensim中word2vec的skip-gram，相关使用请看<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus">官网</a>，关于Gensim 4.0的相关更新请看<a href="https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4">这里</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> word2vec<span class="token keyword">def</span> <span class="token function">train_word2vec</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 训练 word to vector 的 word embedding</span>    model <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>x<span class="token punctuation">,</span> vector_size<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> sg<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> model<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"loading training data ..."</span><span class="token punctuation">)</span>    train_x<span class="token punctuation">,</span> train_y <span class="token operator">=</span> load_data<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> <span class="token string">'train.txt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"loading testing data ..."</span><span class="token punctuation">)</span>    test_x<span class="token punctuation">,</span> test_y <span class="token operator">=</span> load_data<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> <span class="token string">'test.txt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># model</span>    model <span class="token operator">=</span> train_word2vec<span class="token punctuation">(</span>train_x <span class="token operator">+</span> test_x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"saving model ..."</span><span class="token punctuation">)</span>    save_path <span class="token operator">=</span> <span class="token string">'./logs/w2v.model'</span>    model<span class="token punctuation">.</span>save<span class="token punctuation">(</span>save_path<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>loading training data ...loading testing data ...saving model ...</code></pre><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Preprocess</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentences<span class="token punctuation">,</span> sen_len<span class="token punctuation">,</span> w2v_path<span class="token operator">=</span><span class="token string">'w2v.model'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>w2v_path <span class="token operator">=</span> w2v_path        self<span class="token punctuation">.</span>sentences <span class="token operator">=</span> sentences        self<span class="token punctuation">.</span>sen_len <span class="token operator">=</span> sen_len        self<span class="token punctuation">.</span>idx2word <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>word2idx <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>embedding_matrix <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">def</span> <span class="token function">get_w2v_model</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 把之前训练好的 word to vector 模型读进来</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w2v_path<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>vector_size            <span class="token keyword">def</span> <span class="token function">add_embedding</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 把 word 加进 embedding，并赋予它一个随机生成的具有代表性的 vector</span>        <span class="token comment"># word 只会是 "&lt;PAD&gt;" 或 "&lt;UNK&gt;"</span>        vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding_dim<span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>vector<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>word2idx<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>idx2word<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding_matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>embedding_matrix<span class="token punctuation">,</span> vector<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">make_embedding</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> load<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Get embedding ..."</span><span class="token punctuation">)</span>        <span class="token comment"># 取出训练好的 Word2vec word embedding</span>        <span class="token keyword">if</span> load<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"loading word to vec model ..."</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>get_w2v_model<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> NotImplementedError                <span class="token comment"># 制作一个 word2idx 的 dictionary</span>        <span class="token comment"># 制作一个 idx2word 的 list</span>        <span class="token comment"># 制作一个 word2vector 的 list</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>key_to_index<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>word2idx<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>idx2word<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>embedding_matrix<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>get_vector<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding_matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_matrix<span class="token punctuation">)</span>        <span class="token comment"># 将 "&lt;PAD&gt;" 跟 "&lt;UNK&gt;" 加进 embedding 里面</span>        self<span class="token punctuation">.</span>add_embedding<span class="token punctuation">(</span><span class="token string">"&lt;PAD&gt;"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>add_embedding<span class="token punctuation">(</span><span class="token string">"&lt;UNK&gt;"</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"total words: {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_matrix<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embedding_matrix        <span class="token keyword">def</span> <span class="token function">pad_sequence</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 将每个句子变成一样的长度</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>sen_len<span class="token punctuation">:</span>            sentence <span class="token operator">=</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sen_len<span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            pad_len <span class="token operator">=</span> self<span class="token punctuation">.</span>sen_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>pad_len<span class="token punctuation">)</span><span class="token punctuation">:</span>                sentence<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word2idx<span class="token punctuation">[</span><span class="token string">"&lt;PAD&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>sen_len        <span class="token keyword">return</span> sentence        <span class="token keyword">def</span> <span class="token function">sentence_word2idx</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 把句子里面的字转成对应的 index</span>        sentence_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> sen <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>            sentence_idx <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> word <span class="token keyword">in</span> sen<span class="token punctuation">:</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>word <span class="token keyword">in</span> self<span class="token punctuation">.</span>idx2word<span class="token punctuation">)</span><span class="token punctuation">:</span>                    sentence_idx<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    sentence_idx<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word2idx<span class="token punctuation">[</span><span class="token string">"&lt;UNK&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token comment"># 将每个句子变成一样的长度</span>            sentence_idx <span class="token operator">=</span> self<span class="token punctuation">.</span>pad_sequence<span class="token punctuation">(</span>sentence_idx<span class="token punctuation">)</span>            sentence_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sentence_idx<span class="token punctuation">)</span>        <span class="token keyword">return</span> sentence_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">LSTM_Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> fix_embedding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        embedding: 词典        embedding_dim: 词向量的维度        hidden_dim: GRU神经元个数        num_layers: GRU的层数        output_dim: 隐藏层输出的维度(分类的数量)        """</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>LSTM_Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 制作 embedding layer</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span>        <span class="token comment"># 如果 fix_embedding 为 False，在训练过程中，embedding 也会跟着被训练</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token keyword">if</span> fix_embedding <span class="token keyword">else</span> <span class="token boolean">True</span>        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> embedding<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span> bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 句子最后时刻的hidden state</span>        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>制作dataset以便dataloader能够使用</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader<span class="token keyword">class</span> <span class="token class-name">NewsTitleDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>label <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>label<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>在定义dataloder前我们先定义一些模型超参数，该参数可以自行调节</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">sen_len <span class="token operator">=</span> <span class="token number">12</span>batch_size <span class="token operator">=</span> <span class="token number">128</span>n_epoch <span class="token operator">=</span> <span class="token number">10</span>lr <span class="token operator">=</span> <span class="token number">0.001</span>train_preprocess <span class="token operator">=</span> Preprocess<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span> sen_len<span class="token punctuation">,</span> w2v_path<span class="token operator">=</span>save_path<span class="token punctuation">)</span>embedding <span class="token operator">=</span> train_preprocess<span class="token punctuation">.</span>make_embedding<span class="token punctuation">(</span>load<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>train_x <span class="token operator">=</span> train_preprocess<span class="token punctuation">.</span>sentence_word2idx<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 将 data 划分为 training data 和 validation data</span>X_train<span class="token punctuation">,</span> X_val<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_val <span class="token operator">=</span> train_x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">180000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> train_x<span class="token punctuation">[</span><span class="token number">180000</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> train_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">180000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> train_y<span class="token punctuation">[</span><span class="token number">180000</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment"># 将 data 做成 dataset 供 dataloader 使用</span>train_dataset <span class="token operator">=</span> NewsTitleDataset<span class="token punctuation">(</span>X<span class="token operator">=</span>X_train<span class="token punctuation">,</span> y<span class="token operator">=</span>y_train<span class="token punctuation">)</span>val_dataset <span class="token operator">=</span> NewsTitleDataset<span class="token punctuation">(</span>X<span class="token operator">=</span>X_val<span class="token punctuation">,</span> y<span class="token operator">=</span>y_val<span class="token punctuation">)</span><span class="token comment"># 将 data 转成 batch of tensor</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>val_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>val_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Get embedding ...loading word to vec model ...total words: 30360</code></pre><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> time<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># 加载模型</span>model <span class="token operator">=</span> LSTM_Net<span class="token punctuation">(</span>embedding<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">150</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> fix_embedding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># criterion</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># optimizer</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token comment"># Training</span>best_acc <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    epoch_start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_acc<span class="token punctuation">,</span> train_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>    val_acc<span class="token punctuation">,</span> val_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment"># 训练模式</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 前向传播</span>        train_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 后向传播</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>train_pred<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                train_acc <span class="token operator">+=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>train_pred<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        train_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>           model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment"># 评估模型</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            val_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>val_pred<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                        val_acc <span class="token operator">+=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>val_pred<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            val_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 展示结果</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>n_epoch<span class="token punctuation">}</span></span><span class="token string">] </span><span class="token interpolation"><span class="token punctuation">{</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>epoch_start_time<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> sec(s) Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>train_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> Loss: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>train_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string"> | Val Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>val_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_val<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>val_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_val<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>val_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_val<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> best_acc<span class="token punctuation">:</span>        best_acc <span class="token operator">=</span> val_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_val<span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'./logs/lstm_cl.model'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'saving model with acc </span><span class="token interpolation"><span class="token punctuation">{</span>best_acc<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Epoch [1/10] 12.5952 sec(s) Train Acc: 0.8458 Loss: 0.003683 | Val Acc: 0.8786 Loss: 0.003035saving model with acc 0.8786Epoch [2/10] 12.6020 sec(s) Train Acc: 0.8907 Loss: 0.002629 | Val Acc: 0.8906 Loss: 0.002654saving model with acc 0.8906Epoch [3/10] 12.5632 sec(s) Train Acc: 0.8991 Loss: 0.002396 | Val Acc: 0.8902 Loss: 0.002658Epoch [4/10] 12.5930 sec(s) Train Acc: 0.9050 Loss: 0.002251 | Val Acc: 0.8951 Loss: 0.002480saving model with acc 0.8951Epoch [5/10] 12.5931 sec(s) Train Acc: 0.9102 Loss: 0.002110 | Val Acc: 0.8978 Loss: 0.002456saving model with acc 0.8978Epoch [6/10] 12.7861 sec(s) Train Acc: 0.9154 Loss: 0.001967 | Val Acc: 0.8989 Loss: 0.002452saving model with acc 0.8989Epoch [7/10] 12.8262 sec(s) Train Acc: 0.9212 Loss: 0.001838 | Val Acc: 0.9004 Loss: 0.002465saving model with acc 0.9004Epoch [8/10] 12.7670 sec(s) Train Acc: 0.9252 Loss: 0.001716 | Val Acc: 0.9012 Loss: 0.002431saving model with acc 0.9012Epoch [9/10] 14.5936 sec(s) Train Acc: 0.9303 Loss: 0.001590 | Val Acc: 0.9006 Loss: 0.002502Epoch [10/10] 19.8384 sec(s) Train Acc: 0.9358 Loss: 0.001462 | Val Acc: 0.9018 Loss: 0.002567saving model with acc 0.9018</code></pre><p>经过10次迭代后，模型在验证集中的准确度达到0.9018，说明模型还不错，大家可以尝试改动模型参数，看看预测准确度会不会进一步提高。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序列模型</title>
      <link href="/2022/03/20/xu-lie-mo-xing/"/>
      <url>/2022/03/20/xu-lie-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>假设某个序列我们可以使用$x_{t-1},…,x_{t-\tau}$而不是$x_{t-1},…,x_{1}$来估计$x_{t}$，我们就说该序列满足马尔可夫条件。用数学公式表示为：<br>$$P(x_{1},…,x_{T})=\underset{t=1}{\overset{T}{\Pi}}P(x_{t}|x_{t-1},…,x_{t-\tau})$$<br>而$\tau=1$，我们就得到一个一阶马尔可夫模型，则上式变为：<br>$$P(x_{1},…,x_{T})=\underset{t=1}{\overset{T}{\Pi}}P(x_{t}|x_{t-1})$$<br>利用这一事实，我们只需要考虑过去观察中的一个非常短的历史：$P(x_{t}|x_{t-1},…,x_{1})=P(x_{t}|x_{t-1})$就能近似得出当前状态。</p><p>现在我们做一个简单的实验，来探讨一下满足马尔可夫条件的模型预测的准确性，以及它的最大预测能力。</p><h2 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h2><p>首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列模型，时间步长为1000。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchT <span class="token operator">=</span> <span class="token number">1000</span>    <span class="token comment"># 总共产生1000个点</span>time_epoch <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token number">0.01</span> <span class="token operator">*</span> time_epoch<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>T<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>绘制图片</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">,</span> y<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'time_epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/20/xu-lie-mo-xing/output_3_1.png" class=""><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>接下来我们对序列进行一定的处理，使这个序列转换为模型的“特征-标签”对，这里我们使用的$\tau=4$，由于前4个数据没有历史数据来描述它们，因此我们将其舍去。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span class="token keyword">class</span> <span class="token class-name">DigitDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y        <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        <span class="token keyword">return</span> X<span class="token punctuation">,</span> Y    tau <span class="token operator">=</span> <span class="token number">4</span>features <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T <span class="token operator">-</span> tau<span class="token punctuation">,</span> tau<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>tau<span class="token punctuation">)</span><span class="token punctuation">:</span>    features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span>i<span class="token punctuation">:</span> T <span class="token operator">-</span> tau <span class="token operator">+</span> i<span class="token punctuation">]</span>labels <span class="token operator">=</span> y<span class="token punctuation">[</span>tau<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># print(features.shape, labels.shape)</span>batch_size<span class="token punctuation">,</span> n_train <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">600</span>          <span class="token comment"># 仅使用前600个数据进行模型训练</span>train_set <span class="token operator">=</span> DigitDataset<span class="token punctuation">(</span>features<span class="token punctuation">[</span><span class="token punctuation">:</span> n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span> n_train<span class="token punctuation">]</span><span class="token punctuation">)</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><p>在这里我们构建一个十分简单的网络来训练模型：一个拥有两个全连接层的多层感知机，ReLU激活函数，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment"># 初始化网络权重参数</span><span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>设置超参数以及optimizer， criterion，并进行训练</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 模型</span>model <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>n_epoch <span class="token operator">=</span> <span class="token number">10</span>lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token comment"># optimizer</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token comment"># criterion</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># Training</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    total_loss <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span>data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 前向传播</span>        y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 后向传播</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                total_loss <span class="token operator">+=</span> loss <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>        <span class="token comment"># 结果展示</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>n_epoch<span class="token punctuation">:</span><span class="token format-spec">02d</span><span class="token punctuation">}</span></span><span class="token string">] Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>total_loss <span class="token operator">/</span> n_train<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Epoch [1/10] Loss: 0.054475486278533936Epoch [2/10] Loss: 0.015365694649517536Epoch [3/10] Loss: 0.011502934619784355Epoch [4/10] Loss: 0.006994950119405985Epoch [5/10] Loss: 0.007959885522723198Epoch [6/10] Loss: 0.008115933276712894Epoch [7/10] Loss: 0.009392841719090939Epoch [8/10] Loss: 0.008084502071142197Epoch [9/10] Loss: 0.0074587250128388405Epoch [10/10] Loss: 0.006592489313334227</code></pre><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>由训练误差可知，模型运行的效果不错，现在让我们检验模型的预测能力，首先检验模型预测下一个时间步的能力</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">onestep_preds <span class="token operator">=</span> model<span class="token punctuation">(</span>features<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">,</span> y<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">[</span>tau<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> onestep_preds<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> color <span class="token operator">=</span> <span class="token string">'r'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'time_epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token string">'1-step preds'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/20/xu-lie-mo-xing/output_11_1.png" class=""><p>由图可以看出。单步预测效果不错。即使预测的时间步超过了600+4（n_train + tau)，其预测结果看起来仍然不错。但如果数据观察序列只到了604，后面的都需要我们进行预测，那么这个模型的结果将会成为什么样子？还会有这么好的预测效果吗？</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">multistep_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>T<span class="token punctuation">)</span>multistep_preds<span class="token punctuation">[</span><span class="token punctuation">:</span> n_train <span class="token operator">+</span> tau<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span> n_train <span class="token operator">+</span> tau<span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_train <span class="token operator">+</span> tau<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>    multistep_preds<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> model<span class="token punctuation">(</span>multistep_preds<span class="token punctuation">[</span>i <span class="token operator">-</span> tau<span class="token punctuation">:</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 绘图</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">,</span> y<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">[</span>tau<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> onestep_preds<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">,</span> multistep_preds<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'time_epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token string">'1-step preds'</span><span class="token punctuation">,</span> <span class="token string">'multistep preds'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/20/xu-lie-mo-xing/output_13_1.png" class=""><p>由图可以看出绿线的预测显然不是很理想，经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。这其实是错误累积的结果，因此后面误差会相当快地偏离真实的观测结果。<br>现在我们将预测步数分别设置为1，4，16，64，通过对比比较，看看k步预测的困难。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">max_steps <span class="token operator">=</span> <span class="token number">64</span>features <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T <span class="token operator">-</span> tau <span class="token operator">-</span> max_steps <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> tau <span class="token operator">+</span> max_steps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 列i（i&lt;tau）是来⾃x的观测，其时间步从（i+1）到（i+T-tau-max_steps+1）</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>tau<span class="token punctuation">)</span><span class="token punctuation">:</span>    features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span>i <span class="token punctuation">:</span> T <span class="token operator">-</span> tau <span class="token operator">-</span>max_steps <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> i<span class="token punctuation">]</span><span class="token comment"># 列i（i&gt;=tau）是来⾃（i-tau+1）步的预测，其时间步从（i+1）到（i+T-tau-max_steps+1）</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>tau<span class="token punctuation">,</span> tau <span class="token operator">+</span> max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>    features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> model<span class="token punctuation">(</span>features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i <span class="token operator">-</span> tau<span class="token punctuation">:</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>steps <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token comment"># 绘图</span><span class="token keyword">for</span> i <span class="token keyword">in</span> steps<span class="token punctuation">:</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>time_epoch<span class="token punctuation">[</span>tau <span class="token operator">+</span> i <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span> T <span class="token operator">-</span> max_steps <span class="token operator">+</span> i<span class="token punctuation">]</span> <span class="token punctuation">,</span> features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> tau <span class="token operator">+</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'time_epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">-step preds'</span></span> <span class="token keyword">for</span> i <span class="token keyword">in</span> steps<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/20/xu-lie-mo-xing/output_15_1.png" class=""><p>以上例子清楚地说明随着预测步数的增加，预测的结果逐渐变坏。虽然“4步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据操作</title>
      <link href="/2022/03/18/shu-ju-cao-zuo/"/>
      <url>/2022/03/18/shu-ju-cao-zuo/</url>
      
        <content type="html"><![CDATA[<p>首先，我们介绍n维数组，也称为张量（tensor）。在python中数组通过调用Numpy计算包实现，但在pyTorch中为Tensor。虽然这两者相似，但Tensor比Numpy多一些重要的功能：①GPU算力比CPU高，而Numpy仅支持CPU计算；②Tensor类支持自动微分，更适合深度学习。</p><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><p>这节主要介绍一个基本数值计算工具torch的一些操作，首先我们导入torch，并使用arange创建一个行向量。这个⾏向量包含以0开始的前12个整数，它们默认创建为整数。张量中的每个值都称为张量的 元素（element）。例如，张量 x 中有 12 个元素。除⾮额外指定，新的张量将存储在内存中，并采⽤基于CPU的计算。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</code></pre><p>可以通过张量的shape属性来访问张量的形状。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x<span class="token punctuation">.</span>shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([12])</code></pre><p>下面通过numel()函数获取张量中元素的数量</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>12</code></pre><p>在一些场景中，我们需要改变一个张量的形状而不改变元素数量和元素值，这可以通过reshape()函数实现。例如，可以把张量x从形状为(12,)的行向量转换为形状为(3,4)的矩阵。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>X<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11]])</code></pre><p>如果在改变张量形状前，我们知道目标矩阵的行数或列数，那么可以通过-1来调用reshape()函数自动计算出维度的功能。即我们可以⽤x.reshape(-1,4)或x.reshape(3,-1)来取x.reshape(3,4)。</p><p>有时，我们希望使⽤全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。我们可以创建⼀个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[[0., 0., 0., 0.],         [0., 0., 0., 0.],         [0., 0., 0., 0.]],        [[0., 0., 0., 0.],         [0., 0., 0., 0.],         [0., 0., 0., 0.]]])</code></pre><p>同样，我们可以创建⼀个形状为(2,3,4)的张量，其中所有元素都设置为1。代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[[1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.]],        [[1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.]]])</code></pre><p>有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值，可以通过randn()函数实现。如以下代码创建⼀个形状为（3,4）的张量，其中的每个元素都从均值为0、标准差为1的标准正态分布中随机采样。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[ 1.1445,  0.5344,  1.7990,  0.1128],        [-0.5328,  0.4657,  0.7276,  0.2435],        [ 0.0553,  0.2340,  0.8917, -0.5017]])</code></pre><p>我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[2, 1, 4, 3],        [1, 2, 3, 4],        [4, 3, 2, 1]])</code></pre><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>常⻅的标准算术运算符（+、 -、 *、 /和**），该运算符用于任意具有相同形状的张量间的计算。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>x <span class="token operator">+</span> y<span class="token punctuation">,</span> x <span class="token operator">-</span> y<span class="token punctuation">,</span> x <span class="token operator">*</span> y<span class="token punctuation">,</span> x <span class="token operator">/</span> y<span class="token punctuation">,</span> x <span class="token operator">**</span> y <span class="token comment"># **运算符是求幂运算</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(tensor([ 3.,  4.,  6., 10.]), tensor([-1.,  0.,  2.,  6.]), tensor([ 2.,  4.,  8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1.,  4., 16., 64.]))</code></pre><p>“按元素”⽅式可以应⽤更多的计算，包括像求幂这样的⼀元运算符torch.exp(x)，求正弦值torch.sin(x)等等。</p><p>除按元素计算外，还可以执行线性代数运算，包括向量点积和矩阵乘法。我们也可以把多个张量连接（concatenate）在一起形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连接（沿行轴用dim=0，沿列轴用dim=1），这里需要注意张量的形状。下面我们用代码实现张量连接操作：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(tensor([[ 0.,  1.,  2.,  3.],         [ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.],         [ 2.,  1.,  4.,  3.],         [ 1.,  2.,  3.,  4.],         [ 4.,  3.,  2.,  1.]]), tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</code></pre><p>通过sum()函数对张量中的所有元素进⾏求和，会产⽣⼀个单元素张量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor(66.)</code></pre><h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><p>在上⾯的部分中，我们看到了如何在相同形状的两个张量上执⾏按元素操作。在某些情况下，即使形状不同，我们仍然可以通过调⽤ ⼴播机制（broadcasting mechanism）来执⾏按元素操作。这种机制的⼯作⽅式如下：⾸先，通过适当复制元素来扩展⼀个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对⽣成的数组执⾏按元素操作。如下：a和b分别是3 × 1和1 × 2矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵⼴播为⼀个更⼤的3 × 2矩阵，矩阵a将复制列，矩阵b将复制⾏，然后再按元素相加。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>a<span class="token punctuation">,</span> b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(tensor([[0],         [1],         [2]]), tensor([[0, 1]]))</code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">+</span> b<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[0, 1],        [1, 2],        [2, 3]])</code></pre><h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><p>就像python数组一样，张量中的元素可以通过索引访问。与python数组一样，张量中第一个元素的索引是0，最后一个元素的索引是-1。<br>如下所⽰，我们可以⽤[-1]选择最后⼀个元素，可以⽤[1:3]选择第⼆个和第三个元素（对于矩阵是默认对行进行操作）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.]]))</code></pre><p>如果你想实现矩阵中列的索引，可以通过X[:, a:b]实现，同样对行索引可以通过X[a:b, :]实现</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(tensor([[ 1.,  2.],         [ 5.,  6.],         [ 9., 10.]]), tensor([[ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.]]))</code></pre><p>除读取外，我们还可以通过指定索引来将元素写入矩阵</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">9</span>X<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  9.,  7.],        [ 8.,  9., 10., 11.]])</code></pre><h2 id="转换为其他python对象"><a href="#转换为其他python对象" class="headerlink" title="转换为其他python对象"></a>转换为其他python对象</h2><p>将深度学习框架定义的张量转换为Numpy很容易，反之也同样容易。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> X<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>B <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token builtin">type</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(numpy.ndarray, torch.Tensor)</code></pre><p>要将⼤小为1的张量转换为Python标量，我们可以调⽤item函数或Python的内置函数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a<span class="token punctuation">,</span> a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(tensor([3.5000]), 3.5, 3.5, 3)</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://d2l.ai/">Dive into Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo插入图片</title>
      <link href="/2022/03/17/hexo-cha-ru-tu-pian/"/>
      <url>/2022/03/17/hexo-cha-ru-tu-pian/</url>
      
        <content type="html"><![CDATA[<h2 id="typora设置"><a href="#typora设置" class="headerlink" title="typora设置"></a>typora设置</h2><p>打开typora，选择：文件 - 偏好设置 - 图像 - 插入图片，做如下更改：</p><img src="/2022/03/17/hexo-cha-ru-tu-pian/image-20220331152841489.png" class="" title="image-20220331152841489"><p>该设置会使得当你插入图片时，会生成一个和文件名相同的文件夹，并将图片存入这个文件夹内。</p><h2 id="Hexo设置"><a href="#Hexo设置" class="headerlink" title="Hexo设置"></a>Hexo设置</h2><ul><li>更换插件</li></ul><p>用插件 <code>Hexo-renderer-markdown-it</code> （推荐）代替 <code>Hexo-renderer-marked</code>，执行以下代码：</p><pre class="line-numbers language-none"><code class="language-none">npm uninstall hexo-renderer-marker --save  #卸载 markednpm install hexo-renderer-markdown-it --save  #安装markdown-it<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>更改根目录下的_config.yml 配置</li></ul><pre class="line-numbers language-none"><code class="language-none">post_asset_folder: true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>安装插件<code>hexo-image-link</code></li></ul><pre class="line-numbers language-none"><code class="language-none">npm install hexo-image-link --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>该插件可以实现路径转换，假设：</p><p>文件名: <code>./test.md</code></p><p>图片路径: <code>./test/test.jpg</code></p><p>当插入图片 test.jpg 到 test.md 中时，typora 的引用路径为：</p><pre class="line-numbers language-none"><code class="language-none">![](test/test.jpg)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>而在Hexo发布后的引用路径为：</p><pre class="line-numbers language-none"><code class="language-none">![](test.jpg)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>因此，typora的md文件引入hexo时，应转换路径，即删掉图片路径中的 <code>"test/"</code>部分。若在md文件做上述操作，则md文件不能正常显示图片，而</p><p>hexo部署后可正常显示。为了书写方便，引入插件<code>hexo-image-link</code>即可帮助实现了这种路径转换。实现typora 文件中正常显示的图片，在hexo发布后依旧能正常显示。</p>]]></content>
      
      
      <categories>
          
          <category> 生命在于折腾 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成式对抗网络</title>
      <link href="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/"/>
      <url>/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>GAN的全称为Generative Adversarial Network，翻译成中文就是生成式对抗网络。 在github有个<a href="https://github.com/hindupuravinash/the-gan-zoo">GAN Zoo</a>，它记录了GAN的发展并提供了相关GAN的论文来源和部分GAN模型的实现。下图为GAN的论文数量随时间的变化，由图可以看出自2014年第一篇GAN的论文问世，GAN的数量就以指数增长形式迅速壮大。虽然GAN的种类千变万化，但它们的结构类似，都含有一个生成器（generator）和一个判别器（discriminator）。</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313134629440.jpg" class=""><p>判别器和生成器都是一个神经网络结构，也就是一个黑箱模型。判别器相对比较好理解，就像一个二分类模型，有一个判别界限去区分样本，从概率的角度分析就是获得样本x属于类别y的概率，是一个条件概率P(y|x)。而生成器是需要生成数据的概率分布，就像高斯分布一样，需要去拟合整个分布，从概率角度分析就是样本x在整个分布中对应的概率。</p><h1 id="GAN的相关理论"><a href="#GAN的相关理论" class="headerlink" title="GAN的相关理论"></a>GAN的相关理论</h1><p>GAN本质上是在做什么事情呢？</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313134650100.png" class=""><p>以图像生成为例，我们假设吧每一个图片看作二维空间中的一个点，并且现有图片会满足于某个数据分布，我们记作$P_{data}(x)$。那么在这个图像分布空间中，实际上只有很小一部分的区域是人脸图像。如上图所示，只有在蓝色区域采样出的点才会看起来像人脸，而在蓝色区域外采样出来的点就不是人脸。而在GAN中我们需要做的就是让机器找到人脸的分布函数，这也是GAN本质上做的事情。</p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>如下图所示，我们需要训练出这样一个生成器，对于一个已知分布的数据z，我们可以通过生成器把数据转化成一个高维向量，它可以表示为一个图片、文本、声音等。只要我们随机输入多个z，就可以生成一个关于数据x的分布，我们把它称作$P_{G}(x)$。而真实数据也对应一个分布$P_{data}(x)$，生成器的目标是使$P_{G}(x)$和$P_{data}(x)$这两个分布越相似越好。我们知道对于回归和分类模型，都有对应的目标函数，我们只需使这个目标函数达到最优，即可得出一个比较好的回归或分类模型。那么对于两个分布，我们可以利用散度（Divergence，简称Div）这个评价指标来衡量两个分布之间的相似性。Div越小就表示两个分布越相似。那么我们可以将Div作为训练G的目标函数，我们的目标是使Div最小。</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135023130.png" class=""><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>现在有一个最关键的问题是，两个分布之间的Div要如何计算出来呢？理论上来说我们不知道$P_{G}(x)$和$P_{data}(x)$是什么，因此Div我们是无法计算的。因此我们需要构建一个新的网络，它的作用是衡量$P_{G}(x)$和$P_{data}(x)$之间的Div，因此我们有了这样一个网络——判别器。</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135053706.png" class=""><p>图中，蓝色星星是从$P_{data}$（真实数据）中采样出的数据，黄色星星是从$P_{G}$（生成的数据）中采样出的数据，现在我们将这两组数据交给判别器，判别器的功能是判别读入的数据是来自$P_{data}$还是$P_{G}$。如果输入数据是$P_{data}$，那么经过判别器后就输出一个较大的值（可以近似理解为输出1）。如果输入数据是$P_{G}$，那么经过判别器后就输出一个较小的值（可以近似理解为输出0）。熟悉分类模型的同学可能就会说判别器不就相当于是一个二分类模型吗？而前面不是说我们是以Div的大小来判断两个分布之间的相似程度。现在我们就用公式推导出Div和二分类模型的目标函数之间的相关性。<br>我们先来看一下判别器的目标函数：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135121210.png" class=""><p>从式子本身理解的话，数据来源于$P_{data}$，D(x)要尽可能大，数据来源于$P_{G}$，D(x)要尽可能小。这样的话$V(G,D)$就越大。以最大化$V(G,D)$为目标函数，就可以使得输入数据是$P_{G}$，经过判别器后就输出一个较小的值，相反则输入一个较大的值。这样就达到了区分读入的数据是来自$P_{data}$还是$P_{G}$的目的。接下来我们将目标函数展开：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135206860.png" class=""><p>假设判别器十分强大，它模拟出的D(x)可以表示任何函数，给定一个x，都有一个D(x)使得表达式$P_{data}(x)logD(x)+P_{G}(x)log(1-D(x))$最大，求导令其为0可以得出：<br>$$D^{<em>}(x)=\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}$$<br>现在把$D^{</em>}(x)$带入到目标函数中得到：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135318679.png" class=""><p>将表达式中分子分母都除于2可得：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135338407.png" class=""><p>这个表达式等价为：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135404583.png" class=""><p>至此我们可以得知通过一定的假设，得出散度的计算类似于二分类器的目标函数的计算。因此可以看出判别器的本质就是一个二分类器，这样就有利于我们后面代码的实现。<br>现在我们再回到生成器，生成器的目的是让生成数据$P_{G}$和真实数据$P_{data}$之间的Div最小，本来Div是没办法计算的，但是现在有了判别器之后，Div变得可以计算了，于是生成器新的目标函数变为：<br> $$G^{*}=arg\underset{G}{min}\underset{D}{V(G,D)}$$<br>至此，GAN就变成了一个求解最小最大值的问题，接下来就可以用最基本的梯度下降法求解这个问题。<br>下面我们用一个完整的伪算法来回顾一下GAN模型训练的整个流程。</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135516142.png" class=""><p>这段伪代码的意思是，首先我们初始化生成器和判别器的参数，接下来规定一个总迭代次数，每轮训练的迭代次数也确定（这个根据数据量大小确定，一般是3-5次为一轮），在每轮训练中，我们先训练判别器，先从真实数据分布$P_{data}(x)$中抽样x，然后从先验分布中抽样z，并通过生成器产生数据$\overset{-}{x}$，接着把x和$\overset{-}{x}$丢入判别器中训练，使得目标函数$\overset{-}{V}$最大；接下来我们训练生成器，从先验分布中抽样新的z，接下来把z丢进生成器中训练，使得目标函数$\overset{-}{V}$最小，其实这一步就是让这一轮训练好的判别器认为生成器生成的是真实的数据。这样循环交替，最终生成器产生的数据$\overset{-}{x}$就会越来越接近真实数据x。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>这部分我将用GAN实现一个动画人脸的生成，利用的模型是DCGAN，它在原始GAN模型的基础上，将生成器和判别器的网络结构换成了当时已经十分成熟的卷积神经网络结构，并对卷积神经网络结构进行一定的调整，克服了原始GAN训练不稳定和梯度消失的问题。具体改变有：</p><ul><li><p>取消所有的pooling层。生成器中使用fractionally strided convolution代替pooling层，判别器中使用strided convolution代替pooling层。</p></li><li><p>在生成器和判别器中都使用批量标准化</p></li><li><p>去除了全连接层</p></li><li><p>生成器中使用ReLU作为激活函数，最后一层使用tanh激活函数</p></li><li><p>判别器中使用LeakyReLU作为激活函数<br>DCGAN的网络结构如下图所示：</p><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/image-20220313135543883.png" class=""><p>现在让我们来实现这一部分，首先我们设置随机种子的个数（这部分直接复制粘贴就好)</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">import</span> torch<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npseed <span class="token operator">=</span> <span class="token number">2022</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed_all<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">False</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接下来我们对图片数据进行处理，这里需要调用一些包</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> glob<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader<span class="token comment"># DataSet</span><span class="token keyword">class</span> <span class="token class-name">AnimeDataSet</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fnames<span class="token punctuation">,</span> transform<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>fnames <span class="token operator">=</span> fnames        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>fnames<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        fname <span class="token operator">=</span> self<span class="token punctuation">.</span>fnames<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        <span class="token comment"># 加载图片</span>        img <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>io<span class="token punctuation">.</span>read_image<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>        <span class="token comment"># 使用transform对图片进行修改和归一化处理</span>        img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token keyword">return</span> img<span class="token comment"># 获取数据</span>dataset_dir <span class="token operator">=</span> <span class="token string">r'E:\软件包\Chrome\机器学习数据\AnimeDataset\faces'</span>fnames <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>dataset_dir<span class="token punctuation">,</span> <span class="token string">'*'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 1. 修改图片尺寸为(64, 64)</span><span class="token comment"># 2. 将数值从 [0, 1] 线性映射到 [-1, 1]</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>ToPILImage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> AnimeDataSet<span class="token punctuation">(</span>fnames<span class="token punctuation">,</span>transform<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>展示一组图片</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltimages <span class="token operator">=</span> <span class="token punctuation">[</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">]</span>grid_img <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>grid_img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_5_2.png" class=""><p>注意我们数据的范围是[-1,1]，因此我们需要将其转换为[0,1]，才能展示出正确的图片</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">images <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">]</span>grid_img <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>grid_img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_7_1.png" class=""><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>现在实现模型的部分，这里实现的是DCGAN。下图为论文中DCGAN的模型架构，为了加快运行速度，我们起始为512，减少模型的参数，这一部分也可以自行修改</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token comment"># 模型参数初始化</span><span class="token keyword">def</span> <span class="token function">weights_init</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    classname <span class="token operator">=</span> m<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__    <span class="token keyword">if</span> classname<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'Conv'</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>        m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span>  <span class="token comment"># 均值为0，标准差为0.02的正态分布</span>    <span class="token keyword">elif</span> classname<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'BatchNorm'</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>        m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span>        m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>             <span class="token comment"># 均值为1，标准差为0.02的正态分布</span><span class="token comment"># 生成器</span><span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">dconv_bn_relu</span><span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">512</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l2_5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            dconv_bn_relu<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            dconv_bn_relu<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            dconv_bn_relu<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>weights_init<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span>y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>l2_5<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        <span class="token keyword">return</span> y<span class="token comment"># 判别器</span><span class="token keyword">class</span> <span class="token class-name">Discriminator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Discriminator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">conv_bn_lrelu</span><span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            conv_bn_lrelu<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            conv_bn_lrelu<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            conv_bn_lrelu<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>                self<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>weights_init<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>ls<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> y    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>设定好超参数，准备好dataloader，model，loss criterion，optimizer</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 设置一些超参数</span>batch_size <span class="token operator">=</span> <span class="token number">64</span>z_dim <span class="token operator">=</span> <span class="token number">100</span>z_sample <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> z_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token comment"># 随机生成100个样本，用于检测模型的训练结果</span>lr <span class="token operator">=</span> <span class="token number">1e-4</span>n_epoch <span class="token operator">=</span> <span class="token number">10</span><span class="token comment"># 生成一个文件目录，用于保存模型结果</span>save_dir <span class="token operator">=</span> <span class="token string">'./logs'</span>os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># dataloader</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># model</span>G <span class="token operator">=</span> Generator<span class="token punctuation">(</span>in_dim<span class="token operator">=</span>z_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>D <span class="token operator">=</span> Discriminator<span class="token punctuation">(</span>in_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>G<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>D<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># loss criterion</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># optimizer</span>opt_G <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>G<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">)</span>opt_D <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>D<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>        imgs <span class="token operator">=</span> data<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>                bs <span class="token operator">=</span> imgs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                <span class="token triple-quoted-string string">"""训练D"""</span>        z <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> z_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        g_imgs <span class="token operator">=</span> G<span class="token punctuation">(</span>z<span class="token punctuation">)</span>                      <span class="token comment"># 生成的概率分布</span>        r_imgs <span class="token operator">=</span> Variable<span class="token punctuation">(</span>imgs<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># 真实数据的概率分布</span>                <span class="token comment"># 对两种数据打标签，真实为1，生成的为0</span>        g_label <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        r_label <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment"># 两种数据经过判别器</span>        g_logits <span class="token operator">=</span> D<span class="token punctuation">(</span>g_imgs<span class="token punctuation">)</span>        r_logits <span class="token operator">=</span> D<span class="token punctuation">(</span>r_imgs<span class="token punctuation">)</span>                <span class="token comment"># 计算D的loss</span>        g_loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>g_logits<span class="token punctuation">,</span> g_label<span class="token punctuation">)</span>        r_loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>r_logits<span class="token punctuation">,</span> r_label<span class="token punctuation">)</span>        loss_D <span class="token operator">=</span> <span class="token punctuation">(</span>g_loss <span class="token operator">+</span> r_loss<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>                <span class="token comment"># 后向传播更新D的模型参数</span>        D<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_D<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        opt_D<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token triple-quoted-string string">"""训练G"""</span>        z <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> z_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        g_imgs <span class="token operator">=</span> G<span class="token punctuation">(</span>z<span class="token punctuation">)</span>                <span class="token comment"># 生成数据经过判别器</span>        g_logits <span class="token operator">=</span> D<span class="token punctuation">(</span>g_imgs<span class="token punctuation">)</span>                <span class="token comment"># 计算loss</span>        loss_G <span class="token operator">=</span> criterion<span class="token punctuation">(</span>g_logits<span class="token punctuation">,</span> r_label<span class="token punctuation">)</span>     <span class="token comment"># 生成器的目的是生成和真实数据一样的分布，因此用的是r_label</span>                <span class="token comment"># 后向传播更新G的模型参数</span>        G<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_G<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        opt_G<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment"># 打印当前模型训练的状态</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\rEpoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>n_epoch<span class="token punctuation">}</span></span><span class="token string">] </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> Loss_D: </span><span class="token interpolation"><span class="token punctuation">{</span>loss_D<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> Loss_G: </span><span class="token interpolation"><span class="token punctuation">{</span>loss_G<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span>    <span class="token comment"># 每进行一次epoch，生成一组图片，用于评估模型训练的情况</span>    G<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    g_imgs_sample <span class="token operator">=</span> <span class="token punctuation">(</span>G<span class="token punctuation">(</span>z_sample<span class="token punctuation">)</span><span class="token punctuation">.</span>data <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>    filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'Epoch_</span><span class="token interpolation"><span class="token punctuation">{</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">03d</span><span class="token punctuation">}</span></span><span class="token string">.jpg'</span></span><span class="token punctuation">)</span>    torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>save_image<span class="token punctuation">(</span>g_imgs_sample<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f' | save samples to </span><span class="token interpolation"><span class="token punctuation">{</span>filename<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>        <span class="token comment"># 展示生成的图片</span>    grid_img <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>g_imgs_sample<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>grid_img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 将G转换成训练模型</span>    G<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 模型保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>G<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'dcgan_g.pth'</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>D<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'dcgan_d.pth'</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">Epoch [1/10] 1115/1115 Loss_D: 0.1780 Loss_G: 3.8462 | save samples to ./logs\Epoch_001.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_1.png" class=""><pre><code>Epoch [2/10] 1115/1115 Loss_D: 0.2923 Loss_G: 3.1647 | save samples to ./logs\Epoch_002.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_3.png" class=""><pre><code>Epoch [3/10] 1115/1115 Loss_D: 0.1190 Loss_G: 4.1651 | save samples to ./logs\Epoch_003.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_5.png" class=""><pre><code>Epoch [4/10] 1115/1115 Loss_D: 0.1803 Loss_G: 4.8173 | save samples to ./logs\Epoch_004.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_7.png" class=""><pre><code>Epoch [5/10] 1115/1115 Loss_D: 0.3816 Loss_G: 5.1071 | save samples to ./logs\Epoch_005.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_9.png" class=""><pre><code>Epoch [6/10] 1115/1115 Loss_D: 0.3021 Loss_G: 5.8084 | save samples to ./logs\Epoch_006.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_11.png" class=""><pre><code>Epoch [7/10] 1115/1115 Loss_D: 0.1996 Loss_G: 1.8779 | save samples to ./logs\Epoch_007.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_13.png" class=""><pre><code>Epoch [8/10] 1115/1115 Loss_D: 0.1340 Loss_G: 2.7159 | save samples to ./logs\Epoch_008.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_15.png" class=""><pre><code>Epoch [9/10] 1115/1115 Loss_D: 0.1774 Loss_G: 3.6487 | save samples to ./logs\Epoch_009.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_17.png" class=""><pre><code>Epoch [10/10] 1115/1115 Loss_D: 0.0604 Loss_G: 4.6149 | save samples to ./logs\Epoch_010.jpg</code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_13_19.png" class=""><p>现在我们就可以利用我们训练好的Generator来随机生成图片</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 模型加载</span>G <span class="token operator">=</span> Generator<span class="token punctuation">(</span>z_dim<span class="token punctuation">)</span>G<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span><span class="token string">'dcgan_g.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>G<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>G<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>n_output <span class="token operator">=</span> <span class="token number">20</span>z_sample <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_output<span class="token punctuation">,</span> z_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>imgs_sample <span class="token operator">=</span> <span class="token punctuation">(</span>G<span class="token punctuation">(</span>z_sample<span class="token punctuation">)</span><span class="token punctuation">.</span>data <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span>filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'result.jpg'</span></span><span class="token punctuation">)</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>save_image<span class="token punctuation">(</span>imgs_sample<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment"># show image</span>grid_img <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>imgs_sample<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>grid_img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/2022/03/16/sheng-cheng-shi-dui-kang-wang-luo/output_15_0.png" class=""><p>虽然图中的动画人物看起来很怪，但也有几分和动画人物相似，并且有的已经非常像了。由于电脑原因，我只把n_epoch设置为10，如果将n_epoch设置大点，我想结果会好点。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://arxiv.org/abs/1406.2661">Goodfellow, Ian, et al. “Generative adversarial nets.”Advances in neural information processing systems27 (2014).</a></p><p>[2] <a href="https://arxiv.org/abs/1511.06434">Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.”arXiv preprint arXiv:1511.06434(2015).</a></p><p>[3] <a href="https://www.youtube.com/watch?v=DMA4MrNieWo">李宏毅youtube课程</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
